# -*- coding: utf-8 -*-
"""Hack the Track.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16oK3C79qy_JvBQhaAz8ZLxEQJAKMR8Qg

# 1. MOUNT GOOGLE DRIVE & SETUP
"""

import pandas as pd
import numpy as np
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# For machine learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error
import xgboost as xgb
import joblib

# For plotting
import matplotlib.pyplot as plt

print("âœ… All libraries imported successfully!")

"""# 2. SET YOUR DATA PATHS"""

# Update these paths to match your Google Drive structure
BASE_PATH = "/content/drive/MyDrive/toyota_data/sonoma/Sonoma/Race 2"

# Define all your file paths
FILE_PATHS = {
    'telemetry': os.path.join(BASE_PATH, "sonoma_telemetry_R2.csv"),
    'lap_times': os.path.join(BASE_PATH, "sonoma_lap_time_R2.csv"),
    'lap_start': os.path.join(BASE_PATH, "sonoma_lap_start_time_R2.csv"),
    'lap_end': os.path.join(BASE_PATH, "sonoma_lap_end_time_R2.csv"),
    'results': os.path.join(BASE_PATH, "03_Provisional_Results_Race 2_Anonymized.CSV"),
    'best_laps': os.path.join(BASE_PATH, "99_Best 10 Laps By Driver_Race 2_Anonymized.CSV"),
    'section_analysis': os.path.join(BASE_PATH, "23_AnalysisEnduranceWithSections_Race 2_Anonymized.CSV"),
    'results_by_class': os.path.join(BASE_PATH, "05_Provisional_Results by Class_Race 2_Anonymized.CSV")
}

print("ğŸ“ File paths set. Checking if files exist...")
for file_type, path in FILE_PATHS.items():
    if os.path.exists(path):
        print(f"âœ… {file_type}: {os.path.basename(path)}")
    else:
        print(f"âŒ {file_type}: NOT FOUND - {os.path.basename(path)}")

"""# 3. DATA PROCESSING CLASS (FOR ACTUAL DATA)"""

# ======================
# 3. DATA PROCESSING CLASS (CORRECTED)
# ======================
class RacingDataProcessor:
    def __init__(self, debug=True):
        self.debug = debug
        self.processed_data = {}

    def log(self, message):
        if self.debug:
            print(f"ğŸ” {message}")

    def extract_vehicle_info(self, vehicle_id):
        """Extract chassis number from vehicle_id like 'GR86-002-002'"""
        if pd.isna(vehicle_id):
            return "UNKNOWN", "000"

        parts = str(vehicle_id).split('-')
        if len(parts) >= 3:
            chassis = f"{parts[0]}-{parts[1]}"  # GR86-002
            car_number = parts[2]
        else:
            chassis = vehicle_id
            car_number = "000"

        return chassis, car_number

    def load_and_validate_telemetry(self, file_path):
        """Load and validate the large telemetry file"""
        self.log(f"Loading telemetry from: {file_path}")

        # Check file size
        file_size_gb = os.path.getsize(file_path) / (1024**3)
        self.log(f"File size: {file_size_gb:.2f} GB")

        # Read in chunks to handle large file
        chunks = []
        chunk_size = 500000

        try:
            for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
                self.log(f"Processing chunk {i+1}...")
                chunks.append(chunk)

            telemetry_df = pd.concat(chunks, ignore_index=True)
            self.log(f"âœ… Telemetry loaded: {telemetry_df.shape}")

            # CORRECTED: Use 'telemetry_name' instead of 'telemetry'
            required_cols = ['vehicle_id', 'timestamp', 'telemetry_name', 'telemetry_value']
            missing_cols = [col for col in required_cols if col not in telemetry_df.columns]
            if missing_cols:
                raise ValueError(f"Missing columns: {missing_cols}")

            return telemetry_df

        except Exception as e:
            self.log(f"âŒ Error loading telemetry: {e}")
            return None

    def process_telemetry_data(self, telemetry_df, track_name, race_number):
        """Process telemetry data into usable format"""
        self.log("Processing telemetry data...")

        # Extract vehicle info
        vehicle_info = telemetry_df['vehicle_id'].apply(self.extract_vehicle_info)
        telemetry_df[['chassis', 'car_number']] = pd.DataFrame(vehicle_info.tolist(), index=telemetry_df.index)

        # Handle timestamp
        telemetry_df['timestamp_dt'] = pd.to_datetime(telemetry_df['timestamp'], errors='coerce')

        # Create time groups (lap proxy) - using 5-second windows for lap-like grouping
        telemetry_df['time_group'] = telemetry_df['timestamp_dt'].dt.round('5s')

        self.log(f"Unique vehicles: {telemetry_df['chassis'].nunique()}")
        self.log(f"Unique time groups: {telemetry_df['time_group'].nunique()}")

        # Sample the data if it's too large for Colab memory
        if len(telemetry_df) > 1000000:
            self.log("Sampling data for memory efficiency...")
            telemetry_df = telemetry_df.sample(1000000, random_state=42)

        # Pivot telemetry data - CORRECTED: Use 'telemetry_name' instead of 'telemetry'
        try:
            pivoted_df = telemetry_df.pivot_table(
                index=['chassis', 'car_number', 'time_group'],
                columns='telemetry_name',
                values='telemetry_value',
                aggfunc='mean'
            ).reset_index()

            self.log(f"âœ… Pivoted telemetry: {pivoted_df.shape}")
            self.log(f"Pivoted columns: {list(pivoted_df.columns)}")

        except Exception as e:
            self.log(f"âŒ Pivot error: {e}")
            pivoted_df = self.manual_pivot(telemetry_df)

        # Add track info
        pivoted_df['track'] = track_name
        pivoted_df['race_number'] = race_number

        return pivoted_df

    def manual_pivot(self, telemetry_df):
        """Manual pivot implementation"""
        self.log("Using manual pivot method...")

        unique_combos = telemetry_df[['chassis', 'car_number', 'time_group']].drop_duplicates()
        telemetry_types = telemetry_df['telemetry_name'].unique()

        result_df = unique_combos.copy()

        for telemetry_type in telemetry_types:
            result_df[telemetry_type] = np.nan

        for telemetry_type in telemetry_types:
            type_data = telemetry_df[telemetry_df['telemetry_name'] == telemetry_type]
            agg_data = type_data.groupby(['chassis', 'car_number', 'time_group'])['telemetry_value'].mean().reset_index()

            for _, row in agg_data.iterrows():
                mask = ((result_df['chassis'] == row['chassis']) &
                       (result_df['car_number'] == row['car_number']) &
                       (result_df['time_group'] == row['time_group']))
                result_df.loc[mask, telemetry_type] = row['telemetry_value']

        return result_df

    def load_lap_times(self, file_path):
        """Load and process lap times"""
        self.log(f"Loading lap times from: {file_path}")

        try:
            lap_df = pd.read_csv(file_path)

            # Extract vehicle info
            vehicle_info = lap_df['vehicle_id'].apply(self.extract_vehicle_info)
            lap_df[['chassis', 'car_number']] = pd.DataFrame(vehicle_info.tolist(), index=lap_df.index)

            # Clean lap times
            lap_times = lap_df[['chassis', 'car_number', 'lap', 'value']].copy()
            lap_times.columns = ['chassis', 'car_number', 'lap_number', 'lap_time']

            # Convert lap_time to numeric, handling errors
            lap_times['lap_time'] = pd.to_numeric(lap_times['lap_time'], errors='coerce')
            lap_times = lap_times.dropna(subset=['lap_time'])

            self.log(f"âœ… Lap times loaded: {len(lap_times)} records")
            return lap_times

        except Exception as e:
            self.log(f"âŒ Error loading lap times: {e}")
            return None

    def load_driver_info(self, file_path):
        """Load driver information from results file"""
        self.log(f"Loading driver info from: {file_path}")

        try:
            for delimiter in [';', ',', '\t']:
                try:
                    results_df = pd.read_csv(file_path, delimiter=delimiter, low_memory=False)
                    self.log(f"âœ… Successfully read with delimiter: '{delimiter}'")
                    break
                except:
                    continue

            # Extract driver info
            driver_cols = ['NUMBER', 'DRIVER_FIRSTNAME', 'DRIVER_SECONDNAME', 'DRIVER_SHORTNAME', 'VEHICLE', 'CLASS']
            available_cols = [col for col in driver_cols if col in results_df.columns]

            driver_info = results_df[available_cols].copy()
            driver_info.columns = [col.lower() for col in available_cols]

            # Ensure car_number is string for consistent merging
            driver_info['number'] = driver_info['number'].astype(str)

            self.log(f"âœ… Driver info loaded: {len(driver_info)} drivers")
            return driver_info

        except Exception as e:
            self.log(f"âŒ Error loading driver info: {e}")
            return None

    def load_best_laps(self, file_path):
        """Load best laps data - FIXED VERSION"""
        self.log(f"Loading best laps from: {file_path}")

        try:
            # Try different delimiters
            for delimiter in [';', ',', '\t']:
                try:
                    best_laps_df = pd.read_csv(file_path, delimiter=delimiter, low_memory=False)
                    self.log(f"âœ… Successfully read best laps with delimiter: '{delimiter}'")
                    break
                except:
                    continue

            # Debug: Check what columns we actually have
            self.log(f"Best laps columns: {list(best_laps_df.columns)}")
            self.log(f"Best laps shape: {best_laps_df.shape}")

            # Calculate consistency metrics - FIXED: Handle string to numeric conversion
            best_lap_cols = [f'BESTLAP_{i}' for i in range(1, 11)]
            available_lap_cols = [col for col in best_lap_cols if col in best_laps_df.columns]

            if not available_lap_cols:
                self.log("âŒ No best lap columns found")
                # Try alternative column names
                alternative_cols = [col for col in best_laps_df.columns if 'BEST' in col.upper() or 'LAP' in col.upper()]
                self.log(f"Alternative lap columns found: {alternative_cols}")
                available_lap_cols = alternative_cols[:10]  # Take first 10

            self.log(f"Using lap columns: {available_lap_cols}")

            # Create clean dataframe with car number
            best_laps_clean = best_laps_df[['NUMBER'] + available_lap_cols].copy()
            best_laps_clean.columns = ['car_number'] + [f'best_lap_{i}' for i in range(1, len(available_lap_cols)+1)]

            # Convert all lap time columns to numeric, handling errors
            for col in best_laps_clean.columns:
                if col != 'car_number':
                    best_laps_clean[col] = pd.to_numeric(best_laps_clean[col], errors='coerce')

            # Ensure car_number is string for consistent merging
            best_laps_clean['car_number'] = best_laps_clean['car_number'].astype(str)

            # Calculate performance metrics - FIXED: Handle NaN values
            lap_data = best_laps_clean.filter(like='best_lap_')

            # Remove completely NaN rows
            lap_data_clean = lap_data.dropna(how='all')
            if len(lap_data_clean) == 0:
                self.log("âŒ No valid lap data found after cleaning")
                return None

            best_laps_clean['best_lap_time'] = lap_data.min(axis=1, skipna=True)
            best_laps_clean['lap_consistency'] = lap_data.std(axis=1, skipna=True)
            best_laps_clean['avg_best_lap'] = lap_data.mean(axis=1, skipna=True)

            # Remove rows where we couldn't calculate metrics
            best_laps_clean = best_laps_clean.dropna(subset=['best_lap_time', 'lap_consistency', 'avg_best_lap'])

            self.log(f"âœ… Best laps loaded: {len(best_laps_clean)} drivers")
            return best_laps_clean

        except Exception as e:
            self.log(f"âŒ Error loading best laps: {e}")
            import traceback
            self.log(f"Full error: {traceback.format_exc()}")
            return None

    def create_training_dataset(self, file_paths):
        """Create complete training dataset from all files - FIXED MERGING"""
        self.log("Creating training dataset...")

        # 1. Process telemetry data
        telemetry_df = self.load_and_validate_telemetry(file_paths['telemetry'])
        if telemetry_df is not None:
            processed_telemetry = self.process_telemetry_data(telemetry_df, 'sonoma', 2)
        else:
            self.log("âŒ Cannot proceed without telemetry data")
            return None

        # 2. Load lap times (target variable)
        lap_times = self.load_lap_times(file_paths['lap_times'])
        if lap_times is None:
            self.log("âŒ Cannot proceed without lap times")
            return None

        # 3. Load additional data sources
        driver_info = self.load_driver_info(file_paths['results'])
        best_laps = self.load_best_laps(file_paths['best_laps'])

        # 4. Merge all data - FIXED: More robust merging
        self.log("Merging all data sources...")

        # Start with lap times as base
        training_data = lap_times.copy()

        # Ensure car_number is string in base data
        training_data['car_number'] = training_data['car_number'].astype(str)

        # Merge with telemetry (aggregate by chassis/car)
        telemetry_columns_to_agg = []
        expected_telemetry_params = ['accx_can', 'accy_can', 'ath', 'pbrake_r', 'pbrake_f', 'gear', 'Steering_Angle', 'nmot', 'Speed']

        # Only aggregate columns that actually exist in the pivoted data
        for param in expected_telemetry_params:
            if param in processed_telemetry.columns:
                telemetry_columns_to_agg.append(param)

        if not telemetry_columns_to_agg:
            self.log("âŒ No telemetry parameters found in pivoted data")
            return None

        self.log(f"Aggregating telemetry parameters: {telemetry_columns_to_agg}")

        aggregation_dict = {}
        for col in telemetry_columns_to_agg:
            aggregation_dict[col] = ['mean', 'std', 'max']

        telemetry_agg = processed_telemetry.groupby(['chassis', 'car_number']).agg(aggregation_dict).reset_index()

        # Flatten column names
        telemetry_agg.columns = ['_'.join(col).strip('_') for col in telemetry_agg.columns.values]
        telemetry_agg.rename(columns={'chassis_': 'chassis', 'car_number_': 'car_number'}, inplace=True)

        # Ensure car_number is string for merging
        telemetry_agg['car_number'] = telemetry_agg['car_number'].astype(str)

        # Merge telemetry data
        training_data = training_data.merge(telemetry_agg, on=['chassis', 'car_number'], how='left')

        # Merge driver info if available
        if driver_info is not None:
            self.log("Merging driver info...")
            # Ensure consistent column name for merging
            if 'number' in driver_info.columns:
                driver_info = driver_info.rename(columns={'number': 'car_number'})
            driver_info['car_number'] = driver_info['car_number'].astype(str)
            training_data = training_data.merge(driver_info, on='car_number', how='left')

        # Merge best laps if available - FIXED: Check if best_laps exists and has data
        if best_laps is not None and len(best_laps) > 0:
            self.log("Merging best laps...")
            best_laps['car_number'] = best_laps['car_number'].astype(str)

            # Only merge columns that don't already exist
            best_laps_cols_to_merge = [col for col in best_laps.columns if col not in training_data.columns or col == 'car_number']
            best_laps_subset = best_laps[best_laps_cols_to_merge]

            training_data = training_data.merge(best_laps_subset, on='car_number', how='left')
            self.log(f"âœ… Merged best laps data with {len(best_laps_subset.columns)} columns")
        else:
            self.log("âš ï¸ No best laps data to merge")

        # Add engineered features
        training_data = self.add_engineered_features(training_data)

        self.log(f"âœ… Final training dataset: {training_data.shape}")
        self.log(f"Final columns: {list(training_data.columns)}")

        return training_data

    def add_engineered_features(self, df):
        """Add engineered features for better prediction"""
        self.log("Adding engineered features...")

        available_columns = df.columns.tolist()

        if 'accy_can_mean' in available_columns:
            df['cornering_aggression'] = df['accy_can_max']

        if 'pbrake_f_mean' in available_columns and 'pbrake_f_max' in available_columns:
            df['braking_intensity'] = df['pbrake_f_max'] / (df['pbrake_f_mean'] + 0.001)

        if 'ath_mean' in available_columns and 'ath_std' in available_columns:
            df['throttle_consistency'] = 1 / (df['ath_std'] + 0.001)

        if 'Speed_mean' in available_columns and 'Speed_max' in available_columns:
            df['speed_efficiency'] = df['Speed_mean'] / (df['Speed_max'] + 0.001)

        # Performance potential
        if 'best_lap_time' in available_columns and 'lap_time' in available_columns:
            df['performance_ratio'] = df['best_lap_time'] / df['lap_time']

        # Track features
        df['track_length'] = 4.05
        df['track_corners'] = 12
        df['track_elevation'] = 50

        return df

"""# 4. RUN THE DATA PROCESSING"""

# ======================
# 4. RUN THE DATA PROCESSING
# ======================
print("\n" + "="*50)
print("ğŸš€ PROCESSING YOUR ACTUAL DATA")
print("="*50)

processor = RacingDataProcessor(debug=True)
training_data = processor.create_training_dataset(FILE_PATHS)

if training_data is not None:
    print(f"\nâœ… SUCCESS! Processed {len(training_data)} records")
    print(f"Dataset shape: {training_data.shape}")

    # ======================
    # ğŸ•’ LAP TIME SCALING FIX
    # ======================
    print("\nğŸ•’ CHECKING LAP TIME SCALING...")
    current_mean = training_data['lap_time'].mean()

    if current_mean > 1000:
        print("ğŸ”§ Converting lap times from milliseconds to seconds...")
        training_data['lap_time'] = training_data['lap_time'] / 1000
        if 'best_lap_time' in training_data.columns:
            training_data['best_lap_time'] = training_data['best_lap_time'] / 1000
        print(f"âœ… Fixed - New mean: {training_data['lap_time'].mean():.2f}s")

    # ======================
    # ğŸ“Š LAP TIME DISTRIBUTION ANALYSIS (ORIGINAL DATA)
    # ======================
    def analyze_lap_time_distribution(df):
        """Understand why MAE is so high"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))

        # 1. Lap time distribution
        axes[0].hist(df['lap_time'], bins=50, alpha=0.7, edgecolor='black')
        axes[0].axvline(df['lap_time'].mean(), color='r', linestyle='--', label=f'Mean: {df["lap_time"].mean():.1f}s')
        axes[0].axvline(df['lap_time'].median(), color='g', linestyle='--', label=f'Median: {df["lap_time"].median():.1f}s')
        axes[0].set_xlabel('Lap Time (s)')
        axes[0].set_ylabel('Frequency')
        axes[0].set_title('Lap Time Distribution')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # 2. Lap time by lap number (to see patterns)
        if 'lap_number' in df.columns:
            axes[1].scatter(df['lap_number'], df['lap_time'], alpha=0.6, s=30)
            axes[1].set_xlabel('Lap Number')
            axes[1].set_ylabel('Lap Time (s)')
            axes[1].set_title('Lap Time vs Lap Number')
            axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        # Statistical summary
        print("\nğŸ“Š LAP TIME STATISTICS:")
        print(f"   Mean: {df['lap_time'].mean():.2f}s")
        print(f"   Median: {df['lap_time'].median():.2f}s")
        print(f"   Std: {df['lap_time'].std():.2f}s")
        print(f"   Min: {df['lap_time'].min():.2f}s")
        print(f"   Max: {df['lap_time'].max():.2f}s")
        print(f"   Range: {df['lap_time'].max() - df['lap_time'].min():.2f}s")
        print(f"   Coefficient of Variation: {(df['lap_time'].std()/df['lap_time'].mean()*100):.1f}%")

    print("\nğŸ“Š ANALYZING ORIGINAL DATA DISTRIBUTION...")
    analyze_lap_time_distribution(training_data)

    # ======================
    # ğŸ§¹ ENHANCED DATA CLEANING
    # ======================
    print("\n" + "="*50)
    print("ğŸ§¹ ENHANCED DATA CLEANING")
    print("="*50)

    def enhanced_clean_training_data(df):
        """More aggressive cleaning to remove invalid data"""
        original_count = len(df)
        print(f"ğŸ§¹ Starting enhanced cleaning with {original_count} records...")

        # 1. Remove physically impossible lap times
        df_clean = df[df['lap_time'] > 10]  # Remove laps under 10 seconds (impossible)
        removed_impossible = original_count - len(df_clean)

        # 2. Remove laps that are too slow (likely incidents or pit stops)
        # For Sonoma, reasonable lap times are probably 90-130 seconds
        df_clean = df_clean[df_clean['lap_time'] < 180]  # Remove extremely slow laps
        removed_slow = len(df) - len(df_clean) - removed_impossible

        # 3. Focus on the main distribution (remove outliers)
        Q1 = df_clean['lap_time'].quantile(0.10)  # More aggressive
        Q3 = df_clean['lap_time'].quantile(0.90)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.0 * IQR  # Tighter bounds
        upper_bound = Q3 + 1.0 * IQR

        df_clean = df_clean[(df_clean['lap_time'] >= lower_bound) &
                           (df_clean['lap_time'] <= upper_bound)]
        removed_outliers = (original_count - removed_impossible - removed_slow) - len(df_clean)

        # 4. Remove laps with missing or zero telemetry
        critical_features = ['accy_can_mean', 'nmot_mean', 'ath_mean', 'Speed_mean']
        available_critical = [f for f in critical_features if f in df_clean.columns]

        for feature in available_critical:
            before = len(df_clean)
            df_clean = df_clean[(df_clean[feature] != 0) & (df_clean[feature].notna())]
            removed_zero = before - len(df_clean)
            if removed_zero > 0:
                print(f"   Removed {removed_zero} records with zero/invalid {feature}")

        print(f"ğŸ”§ Enhanced Cleaning Report:")
        print(f"   Original records: {original_count}")
        print(f"   Removed {removed_impossible} impossible laps (<10s)")
        print(f"   Removed {removed_slow} extremely slow laps (>180s)")
        print(f"   Removed {removed_outliers} statistical outliers")
        print(f"   Final clean records: {len(df_clean)}")
        print(f"   Total removed: {original_count - len(df_clean)} ({((original_count - len(df_clean))/original_count*100):.1f}%)")

        return df_clean

    # Apply enhanced cleaning
    training_data_enhanced_clean = enhanced_clean_training_data(training_data)

    # ======================
    # ğŸ¯ LAP TYPE ANALYSIS
    # ======================
    def analyze_lap_types(df):
        """Understand what types of laps are in the dataset"""
        print("\nğŸ¯ LAP TYPE ANALYSIS:")

        # Define lap type ranges (adjust based on your track)
        qualifying_range = (df['lap_time'].min(), df['lap_time'].quantile(0.2))
        race_fast_range = (df['lap_time'].quantile(0.2), df['lap_time'].quantile(0.5))
        race_avg_range = (df['lap_time'].quantile(0.5), df['lap_time'].quantile(0.8))
        race_slow_range = (df['lap_time'].quantile(0.8), df['lap_time'].max())

        qualifying_laps = df[(df['lap_time'] >= qualifying_range[0]) & (df['lap_time'] < qualifying_range[1])]
        race_fast_laps = df[(df['lap_time'] >= race_fast_range[0]) & (df['lap_time'] < race_fast_range[1])]
        race_avg_laps = df[(df['lap_time'] >= race_avg_range[0]) & (df['lap_time'] < race_avg_range[1])]
        race_slow_laps = df[(df['lap_time'] >= race_slow_range[0]) & (df['lap_time'] <= race_slow_range[1])]

        print(f"   Qualifying-style laps (<P20): {len(qualifying_laps)} ({len(qualifying_laps)/len(df)*100:.1f}%)")
        print(f"   Fast race laps (P20-P50): {len(race_fast_laps)} ({len(race_fast_laps)/len(df)*100:.1f}%)")
        print(f"   Average race laps (P50-P80): {len(race_avg_laps)} ({len(race_avg_laps)/len(df)*100:.1f}%)")
        print(f"   Slow race laps (>P80): {len(race_slow_laps)} ({len(race_slow_laps)/len(df)*100:.1f}%)")

        # Check if we should focus on specific lap types
        if len(qualifying_laps) / len(df) > 0.3:
            print("   ğŸ’¡ Recommendation: Consider training on qualifying laps separately")
        if len(race_slow_laps) / len(df) > 0.3:
            print("   âš ï¸  Warning: Many slow laps - may want to exclude them")

    # Run lap type analysis on enhanced cleaned data
    print("\nğŸ“Š ANALYZING ENHANCED CLEANED DATA...")
    analyze_lap_time_distribution(training_data_enhanced_clean)
    analyze_lap_types(training_data_enhanced_clean)

    # ======================
    # ğŸ¯ OPTION: FOCUS ON RACING LAPS ONLY
    # ======================
    print("\n" + "="*50)
    print("ğŸ¯ FOCUSING ON REPRESENTATIVE RACING LAPS")
    print("="*50)

    # Train only on the most representative racing laps (middle 80%)
    racing_laps = training_data_enhanced_clean[
        training_data_enhanced_clean['lap_time'].between(
            training_data_enhanced_clean['lap_time'].quantile(0.1),
              training_data_enhanced_clean['lap_time'].quantile(0.9)
        )
    ]

    print(f"ğŸ“Š Racing laps only: {len(racing_laps)} records")
    analyze_lap_time_distribution(racing_laps)

    # # ======================
    # # 5. XGBOOST MODEL TRAINING (WITH ENHANCED CLEANED DATA)
    # # ======================
    # print("\n" + "="*50)
    # print("ğŸ¤– XGBOOST MODEL TRAINING (WITH ENHANCED CLEANED DATA)")
    # print("="*50)

    # # Use the ENHANCED CLEANED data for training
    # # Prepare features
    # X, y, feature_cols = prepare_features_for_training(racing_laps)  # Or use training_data_enhanced_clean
    # model, scaler, X_test, y_test, y_pred = train_xgboost_model(X, y)

    # # Continue with the rest of your code (feature importance, saving model, etc.)

else:
    print("âŒ Failed to process data")

"""# 5. XGBOOST MODEL TRAINING"""

print("\n" + "="*50)
print("ğŸ¤– XGBOOST MODEL TRAINING (WITH ENHANCED CLEANED DATA)")
print("="*50)

def prepare_features_for_training(df):
    """Prepare features for XGBoost training"""

    # Select numeric features (excluding target and identifiers)
    exclude_cols = ['chassis', 'car_number', 'lap_time', 'number', 'driver_firstname',
                   'driver_secondname', 'driver_shortname', 'vehicle', 'class']

    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]

    # Handle missing values
    X = df[feature_cols].copy()
    y = df['lap_time']

    # Fill missing values with median
    for col in X.columns:
        if X[col].isna().any():
            X[col].fillna(X[col].median(), inplace=True)

    print(f"ğŸ“ˆ Using {len(feature_cols)} features for training")
    print(f"Features: {feature_cols}")

    return X, y, feature_cols

def train_xgboost_model(X, y):
    """Train XGBoost model for lap time prediction"""

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)

    print(f"Training set: {X_train.shape}")
    print(f"Test set: {X_test.shape}")

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train XGBoost model
    model = xgb.XGBRegressor(
        objective='reg:squarederror',
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        early_stopping_rounds=50
    )

    print("ğŸ‹ï¸ Training XGBoost model...")
    model.fit(
        X_train_scaled, y_train,
        eval_set=[(X_test_scaled, y_test)],
        verbose=False
    )

    # Make predictions
    y_pred = model.predict(X_test_scaled)

    # Calculate metrics
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"âœ… Model trained successfully!")
    print(f"ğŸ“Š Performance Metrics:")
    print(f"   Mean Absolute Error: {mae:.4f} seconds")
    print(f"   Root Mean Squared Error: {rmse:.4f} seconds")
    print(f"   Baseline (mean): {y.mean():.4f} seconds")

    return model, scaler, X_test_scaled, y_test, y_pred

if training_data is not None:
    # Prepare features
    X, y, feature_cols = prepare_features_for_training(racing_laps)

    # Train model
    model, scaler, X_test, y_test, y_pred = train_xgboost_model(X, y)

    # Feature importance
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\nğŸ” Top 10 Most Important Features:")
    print(feature_importance.head(10))

    # Save the model and scaler
    model.save_model('/content/drive/MyDrive/toyota_data/xgboost_lap_time_model.json')
    joblib.dump(scaler, '/content/drive/MyDrive/toyota_data/scaler.pkl')
    print("ğŸ’¾ Model and scaler saved to Google Drive")

else:
    print("âŒ Cannot train model without processed data")

"""# MAKE FRONTEND DATA"""

# ======================
# ğŸ¯ CREATE FRONTEND DEMO DATASET WITH CALCULATED SPEED METRICS
# ======================
print("\n" + "="*50)
print("ğŸ¯ CREATING FRONTEND DEMO DATASET WITH CALCULATED SPEED METRICS")
print("="*50)

def calculate_speed_metrics(lap_time_seconds, track_length_km=4.05):
    """
    Calculate speed metrics from lap time and track length
    """
    # Convert track length to meters
    track_length_m = track_length_km * 1000

    # Calculate average speed in m/s and km/h
    avg_speed_mps = track_length_m / lap_time_seconds
    avg_speed_kmh = avg_speed_mps * 3.6

    # Estimate top speed (typically 15-25% higher than average for road courses)
    # This is an approximation - real top speed depends on track layout
    top_speed_kmh = avg_speed_kmh * 1.20  # 20% higher than average

    return {
        'avg_speed_mps': avg_speed_mps,
        'avg_speed_kmh': avg_speed_kmh,
        'estimated_top_speed_kmh': top_speed_kmh
    }

def create_enhanced_frontend_dataset(racing_laps, feature_cols, model, scaler, sample_size=2000):
    """
    Create an enhanced demo dataset for frontend testing that includes:
    - Sample telemetry data with predictions
    - Calculated speed metrics from lap times
    - Raw telemetry values for frontend charts
    - Example inputs for the prediction API
    - Expected outputs for validation
    """

    print("ğŸ“Š Creating enhanced frontend demo dataset...")

    # First, let's check what columns actually exist
    available_columns = racing_laps.columns.tolist()
    print(f"Available columns in dataset: {len(available_columns)}")
    print(f"First 20 columns: {available_columns[:20]}")

    # Select a larger, diverse sample of laps for the demo
    sample_size = min(sample_size, len(racing_laps))
    demo_indices = np.random.choice(len(racing_laps), size=sample_size, replace=False)
    demo_data = racing_laps.iloc[demo_indices].copy()

    # Prepare features for prediction
    X_demo = demo_data[feature_cols].copy()

    # Handle missing values (same as training)
    for col in X_demo.columns:
        if X_demo[col].isna().any():
            X_demo[col].fillna(X_demo[col].median(), inplace=True)

    # Make predictions
    X_demo_scaled = scaler.transform(X_demo)
    predictions = model.predict(X_demo_scaled)

    # Create the enhanced demo dataset
    frontend_dataset = pd.DataFrame()

    # Add essential identifiers
    frontend_dataset['demo_id'] = range(len(demo_data))
    frontend_dataset['chassis'] = demo_data['chassis'].values
    frontend_dataset['car_number'] = demo_data['car_number'].values
    frontend_dataset['lap_number'] = demo_data['lap_number'].values

    # Add actual lap times (for validation)
    frontend_dataset['actual_lap_time'] = demo_data['lap_time'].values

    # Add predicted lap times
    frontend_dataset['predicted_lap_time'] = predictions

    # Add prediction error
    frontend_dataset['prediction_error'] = frontend_dataset['actual_lap_time'] - frontend_dataset['predicted_lap_time']

    # Add all the feature columns (this is what the frontend will send for prediction)
    for feature in feature_cols:
        frontend_dataset[feature] = X_demo[feature].values

    # CALCULATE SPEED METRICS FROM LAP TIMES
    print("\nğŸš€ Calculating speed metrics from lap times...")

    # Sonoma Raceway length is approximately 4.05 km
    track_length_km = 4.05

    # Calculate speed metrics for each lap
    speed_data = []
    for lap_time in frontend_dataset['actual_lap_time']:
        speeds = calculate_speed_metrics(lap_time, track_length_km)
        speed_data.append(speeds)

    # Add calculated speed metrics to dataset
    speed_df = pd.DataFrame(speed_data)
    frontend_dataset['calculated_avg_speed_kmh'] = speed_df['avg_speed_kmh']
    frontend_dataset['calculated_avg_speed_mps'] = speed_df['avg_speed_mps']
    frontend_dataset['calculated_top_speed_kmh'] = speed_df['estimated_top_speed_kmh']

    print(f"   âœ… Calculated speed metrics for {len(speed_data)} laps")
    print(f"   ğŸ“Š Speed range: {frontend_dataset['calculated_avg_speed_kmh'].min():.1f} - {frontend_dataset['calculated_avg_speed_kmh'].max():.1f} km/h")

    # ADD RAW TELEMETRY VALUES FOR FRONTEND CHARTS
    print("\nğŸ“ˆ Adding raw telemetry values for frontend visualization...")

    # Define possible telemetry columns and check which ones exist
    possible_telemetry_columns = {
        'speed': ['Speed_mean', 'Speed_std', 'Speed_max', 'speed_mean', 'speed_std', 'speed_max'],
        'acceleration_x': ['accx_can_mean', 'accx_can_std', 'accx_can_max'],
        'acceleration_y': ['accy_can_mean', 'accy_can_std', 'accy_can_max'],
        'throttle': ['ath_mean', 'ath_std', 'ath_max'],
        'brake_front': ['pbrake_f_mean', 'pbrake_f_std', 'pbrake_f_max'],
        'brake_rear': ['pbrake_r_mean', 'pbrake_r_std', 'pbrake_r_max'],
        'rpm': ['nmot_mean', 'nmot_std', 'nmot_max'],
        'steering': ['Steering_Angle_mean', 'Steering_Angle_std', 'Steering_Angle_max'],
        'gear': ['gear_mean', 'gear_std', 'gear_max']
    }

    # Find which telemetry columns actually exist
    existing_telemetry_cols = []
    for category, possible_cols in possible_telemetry_columns.items():
        for col in possible_cols:
            if col in demo_data.columns:
                existing_telemetry_cols.append(col)
                # Add to frontend dataset with raw_ prefix
                frontend_dataset[f'raw_{col}'] = demo_data[col].values
                print(f"   âœ… Added: raw_{col}")
                break  # Only add one from each category if multiple exist

    # Add calculated metrics for charts (with fallbacks)
    print("\nğŸ”§ Adding calculated metrics...")

    # Cornering G-force
    if 'accy_can_max' in demo_data.columns:
        frontend_dataset['raw_cornering_gforce'] = demo_data['accy_can_max'].values
        print("   âœ… Added: raw_cornering_gforce")
    else:
        frontend_dataset['raw_cornering_gforce'] = 0.0
        print("   âš ï¸  Cornering data not available")

    # Braking G-force (use negative of accx_can_max as fallback)
    if 'accx_can_min' in demo_data.columns:
        frontend_dataset['raw_braking_gforce'] = demo_data['accx_can_min'].values
    elif 'accx_can_max' in demo_data.columns:
        frontend_dataset['raw_braking_gforce'] = demo_data['accx_can_max'].values * -1
    else:
        frontend_dataset['raw_braking_gforce'] = 0.0
    print("   âœ… Added: raw_braking_gforce")

    # Use calculated speeds for visualization
    frontend_dataset['raw_top_speed'] = frontend_dataset['calculated_top_speed_kmh']
    frontend_dataset['raw_avg_speed'] = frontend_dataset['calculated_avg_speed_kmh']
    print("   âœ… Using calculated speeds for visualization")

    # RPM peak
    rpm_cols = [col for col in ['nmot_max', 'nmot_mean'] if col in demo_data.columns]
    if rpm_cols:
        frontend_dataset['raw_rpm_peak'] = demo_data[rpm_cols[0]].values
        print(f"   âœ… Added: raw_rpm_peak (from {rpm_cols[0]})")
    else:
        frontend_dataset['raw_rpm_peak'] = 0.0
        print("   âš ï¸  RPM data not available")

    # Throttle usage
    throttle_cols = [col for col in ['ath_mean', 'ath_max'] if col in demo_data.columns]
    if throttle_cols:
        frontend_dataset['raw_throttle_usage'] = demo_data[throttle_cols[0]].values
        print(f"   âœ… Added: raw_throttle_usage (from {throttle_cols[0]})")
    else:
        frontend_dataset['raw_throttle_usage'] = 0.0
        print("   âš ï¸  Throttle data not available")

    # Add lap efficiency metric (higher = more efficient driving)
    frontend_dataset['lap_efficiency'] = frontend_dataset['calculated_avg_speed_kmh'] / frontend_dataset['calculated_top_speed_kmh']

    # Add some metadata
    frontend_dataset['track'] = 'sonoma'
    frontend_dataset['track_length_km'] = track_length_km
    frontend_dataset['session_type'] = 'race'
    frontend_dataset['data_source'] = 'demo'
    frontend_dataset['prediction_confidence'] = 0.95  # Simulated confidence

    print(f"\nâœ… Created enhanced demo dataset with {len(frontend_dataset)} samples")
    print(f"   Features: {len(feature_cols)}")
    print(f"   Raw telemetry columns: {len([col for col in frontend_dataset.columns if 'raw_' in col])}")
    print(f"   Calculated speed metrics: 3 new columns")
    print(f"   Average prediction error: {frontend_dataset['prediction_error'].abs().mean():.3f}s")
    print(f"   Average speed: {frontend_dataset['calculated_avg_speed_kmh'].mean():.1f} km/h")
    print(f"   Estimated top speed: {frontend_dataset['calculated_top_speed_kmh'].mean():.1f} km/h")

    return frontend_dataset



def create_chart_friendly_dataset(frontend_dataset):
    """
    Create a dataset optimized for frontend charts with aggregated statistics
    """
    print("\nğŸ“Š Creating chart-optimized dataset...")

    chart_data = frontend_dataset.copy()

    # Add performance categories for filtering
    lap_time_bins = [0, chart_data['actual_lap_time'].quantile(0.25),
                     chart_data['actual_lap_time'].quantile(0.75), 200]
    chart_data['performance_category'] = pd.cut(
        chart_data['actual_lap_time'],
        bins=lap_time_bins,
        labels=['Elite', 'Good', 'Average']
    )

    # Add speed categories
    speed_bins = [0, chart_data['calculated_avg_speed_kmh'].quantile(0.33),
                  chart_data['calculated_avg_speed_kmh'].quantile(0.66), 200]
    chart_data['speed_category'] = pd.cut(
        chart_data['calculated_avg_speed_kmh'],
        bins=speed_bins,
        labels=['Slow', 'Medium', 'Fast']
    )

    # Add consistency metrics (with fallbacks)
    if 'raw_Speed_std' in chart_data.columns:
        chart_data['speed_consistency'] = 1 / (chart_data['raw_Speed_std'] + 0.1)
    else:
        chart_data['speed_consistency'] = 1.0

    if 'raw_ath_std' in chart_data.columns:
        chart_data['throttle_consistency'] = 1 / (chart_data['raw_ath_std'] + 0.1)
    else:
        chart_data['throttle_consistency'] = 1.0

    if 'raw_pbrake_f_std' in chart_data.columns:
        chart_data['braking_consistency'] = 1 / (chart_data['raw_pbrake_f_std'] + 0.1)
    else:
        chart_data['braking_consistency'] = 1.0

    # Add driving style classification
    if 'raw_cornering_gforce' in chart_data.columns:
        chart_data['driving_style'] = np.where(
            chart_data['raw_cornering_gforce'] > chart_data['raw_cornering_gforce'].median(),
            'Aggressive',
            'Smooth'
        )
    else:
        chart_data['driving_style'] = 'Unknown'

    print(f"âœ… Created chart-optimized dataset with {len(chart_data)} samples")
    print(f"   Speed categories: Slow (<{speed_bins[1]:.1f} km/h), Medium, Fast (>{speed_bins[2]:.1f} km/h)")

    return chart_data

# Create the enhanced frontend datasets
if training_data is not None:
    # First, let's see what we're working with
    print(f"ğŸ“Š Dataset shape: {racing_laps.shape}")
    print(f"ğŸ“Š Available columns: {len(racing_laps.columns)}")
    print(f"ğŸ“Š Sample of columns: {list(racing_laps.columns[:30])}")

    # Create main demo dataset with 2000 samples
    frontend_demo_data = create_enhanced_frontend_dataset(racing_laps, feature_cols, model, scaler, sample_size=2000)



    # Create chart-optimized dataset
    chart_data = create_chart_friendly_dataset(frontend_demo_data)

    # Create API test cases (using a subset)
    api_test_cases = create_api_test_cases(frontend_demo_data.head(100), feature_cols)  # 100 test cases

    # Create feature template
    feature_template = create_feature_template(feature_cols)

    # Fill in example values from the actual data
    for feature in feature_cols:
        if feature in racing_laps.columns:
            feature_template[feature]["example_value"] = float(racing_laps[feature].median())

    # Save all frontend assets
    print("\nğŸ’¾ Saving enhanced frontend demo assets...")

    # 1. Main demo dataset with calculated speeds (2000 samples)
    frontend_demo_data.to_csv('/content/drive/MyDrive/toyota_data/frontend_demo_dataset.csv', index=False)
    print("âœ… Saved: frontend_demo_dataset.csv (2000 samples with calculated speeds)")



    # 3. Chart-optimized dataset
    chart_data.to_csv('/content/drive/MyDrive/toyota_data/chart_dataset.csv', index=False)
    print("âœ… Saved: chart_dataset.csv (optimized for visualization)")

    # 4. API test cases (for development)
    with open('/content/drive/MyDrive/toyota_data/api_test_cases.json', 'w') as f:
        json.dump(api_test_cases, f, indent=2)
    print("âœ… Saved: api_test_cases.json (100 test cases)")

    # 5. Feature schema (for API documentation)
    with open('/content/drive/MyDrive/toyota_data/feature_schema.json', 'w') as f:
        json.dump(feature_template, f, indent=2)
    print("âœ… Saved: feature_schema.json")

    # 6. Minimal test dataset (just 10 samples for quick testing)
    minimal_test_data = frontend_demo_data.head(10)[['demo_id', 'car_number', 'lap_number'] + feature_cols]
    minimal_test_data.to_csv('/content/drive/MyDrive/toyota_data/minimal_test_data.csv', index=False)
    print("âœ… Saved: minimal_test_data.csv (10 quick samples)")



    # Print comprehensive summary
    print(f"\nğŸ¯ ENHANCED FRONTEND ASSETS SUMMARY:")
    print(f"   â€¢ Main demo dataset: {len(frontend_demo_data)} samples")

    print(f"   â€¢ Chart dataset: {len(chart_data)} samples with categories")
    print(f"   â€¢ Features: {len(feature_cols)} parameters")

    # Show what speed and telemetry columns were added
    speed_cols = [col for col in frontend_demo_data.columns if 'speed' in col.lower() or 'calculated' in col]
    raw_cols = [col for col in frontend_demo_data.columns if 'raw_' in col]
    print(f"   â€¢ Speed metrics: {len(speed_cols)} columns")
    print(f"   â€¢ Raw telemetry columns: {len(raw_cols)}")

    print(f"   â€¢ Average prediction error: {frontend_demo_data['prediction_error'].abs().mean():.3f}s")
    print(f"   â€¢ Average speed: {frontend_demo_data['calculated_avg_speed_kmh'].mean():.1f} km/h")

    # Show examples of what the frontend can visualize
    print(f"\nğŸ“Š FRONTEND VISUALIZATION CAPABILITIES:")
    print(f"   ğŸš€ SPEED ANALYSIS:")
    for col in speed_cols:
        print(f"     â€¢ {col}")
    print(f"   ğŸ“ˆ TELEMETRY ANALYSIS:")
    for col in raw_cols[:10]:  # Show first 10
        print(f"     â€¢ {col}")

    print(f"\nğŸš€ Enhanced frontend assets with calculated speeds ready for React visualization!")

# ======================
# ğŸ¯ BUSINESS CONTEXT & STRATEGIC VALUE
# ======================
print("\n" + "="*50)
print("ğŸ’¼ BUSINESS CONTEXT & STRATEGIC VALUE")
print("="*50)

def calculate_strategic_value(model, X_test, y_test, y_pred, feature_cols):
    """Show how accurate predictions create race advantages"""
    from sklearn.metrics import mean_absolute_error, r2_score

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    mean_lap_time = y_test.mean()

    print("ğŸ¯ STRATEGIC ADVANTAGE ANALYSIS:")
    print(f"â€¢ Average prediction error: {mae:.2f}s")
    print(f"â€¢ Error percentage: {(mae/mean_lap_time*100):.1f}%")
    print(f"â€¢ Model confidence (RÂ²): {r2:.3f}")
    print("")
    print("ğŸ RACING STRATEGY IMPLICATIONS:")
    print(f"â€¢ In a 30-lap race, total error: {mae * 30:.1f}s")
    print(f"â€¢ Typical overtaking opportunity: 1-2 seconds")
    print(f"â€¢ Pit stop window precision needed: Â±3 seconds")
    print(f"â€¢ Qualifying lap margin: 0.1-0.5s")
    print("")
    print("âœ… TACTICAL APPLICATIONS:")
    print("   â€¢ Pit stop timing optimization")
    print("   â€¢ Tire strategy decisions")
    print("   â€¢ Fuel load planning")
    print("   â€¢ Overtaking opportunity identification")
    print("   â€¢ Driver performance monitoring")
    print("")

    # Performance grading
    error_percentage = (mae/mean_lap_time*100)
    if error_percentage < 1.0:
        grade = "ğŸ‰ ELITE - F1 Level"
    elif error_percentage < 2.0:
        grade = "âœ… EXCELLENT - Professional Racing"
    elif error_percentage < 3.0:
        grade = "ğŸ‘ GOOD - Competitive Racing"
    else:
        grade = "âš ï¸  NEEDS IMPROVEMENT"

    print(f"ğŸ“Š PERFORMANCE GRADE: {grade}")
    print(f"   Your error rate: {error_percentage:.1f}%")

# Run business context analysis
calculate_strategic_value(model, X_test, y_test, y_pred, feature_cols)

# ======================
# ğŸ’¾ SAVE FINAL MODEL & ARTIFACTS
# ======================
print("\n" + "="*50)
print("ğŸ’¾ SAVING FINAL MODEL & ARTIFACTS")
print("="*50)

# Save the final model
model.save_model('/content/drive/MyDrive/toyota_data/final_xgboost_model.json')
joblib.dump(scaler, '/content/drive/MyDrive/toyota_data/final_scaler.pkl')

# Save feature list
feature_info = {
    'features': feature_cols,
    'feature_count': len(feature_cols),
    'performance_mae': mean_absolute_error(y_test, y_pred),
    'performance_rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
}

import json
with open('/content/drive/MyDrive/toyota_data/model_metadata.json', 'w') as f:
    json.dump(feature_info, f, indent=2)

print("âœ… Final model and artifacts saved:")
print(f"   â€¢ Model: final_xgboost_model.json")
print(f"   â€¢ Scaler: final_scaler.pkl")
print(f"   â€¢ Metadata: model_metadata.json")
print(f"   â€¢ Features: {len(feature_cols)}")
print(f"   â€¢ Final MAE: {mean_absolute_error(y_test, y_pred):.3f}s")

"""# 6. PREDICTION FUNCTION"""

print("\n" + "="*50)
print("ğŸ”® PREDICTION PIPELINE")
print("="*50)

def predict_lap_time(new_data, model, scaler, feature_cols):
    """Predict lap time for new data"""

    # Ensure we have the same features as training
    missing_features = set(feature_cols) - set(new_data.columns)
    extra_features = set(new_data.columns) - set(feature_cols)

    if missing_features:
        print(f"âš ï¸  Adding missing features: {missing_features}")
        for feature in missing_features:
            new_data[feature] = 0  # Or use median from training

    if extra_features:
        print(f"âš ï¸  Removing extra features: {extra_features}")
        new_data = new_data[feature_cols]

    # Handle missing values
    for col in feature_cols:
        if col in new_data.columns and new_data[col].isna().any():
            new_data[col].fillna(new_data[col].median(), inplace=True)

    # Scale and predict
    X_new = scaler.transform(new_data[feature_cols])
    predictions = model.predict(X_new)

    return predictions

# Example of how to use the prediction function
if training_data is not None:
    # Use first row as example new data
    example_new_data = X.iloc[[0]].copy()
    predicted_time = predict_lap_time(example_new_data, model, scaler, feature_cols)

    print(f"ğŸ¯ Example Prediction:")
    print(f"   Actual lap time: {y.iloc[0]:.3f} seconds")
    print(f"   Predicted lap time: {predicted_time[0]:.3f} seconds")
    print(f"   Error: {abs(y.iloc[0] - predicted_time[0]):.3f} seconds")

"""# 7. NEXT STEPS"""

print("\n" + "="*50)
print("ğŸ¯ NEXT STEPS")
print("="*50)

print("""
1. âœ… DATA PROCESSING: Complete for Sonoma Race 2
2. âœ… MODEL TRAINING: XGBoost trained on processed data
3. ğŸ”„ SCALE TO OTHER TRACKS: Repeat for other races/tracks
4. ğŸš€ DEPLOY: Use the prediction function for new data

To process other tracks, update the BASE_PATH and run again:
BASE_PATH = "/content/drive/MyDrive/toyota_data/indianapolis/Race 1"

You can also combine multiple tracks by processing each separately and then concatenating the results.
""")

print("\nğŸ‰ PIPELINE COMPLETE! Your racing prediction model is ready!")